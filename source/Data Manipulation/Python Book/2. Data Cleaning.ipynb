{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f536883",
   "metadata": {},
   "source": [
    "- note for me: what I think is relevant to include: Check for outliers, redundancy, duplicates, missing values and how to deal with them #unique, remove duplicates, Handling Missing Data, dropna, libraries: pandas, numpy, scikit learn\n",
    "\n",
    "\n",
    "<span style=\"color:darkred\"> :Explanation: this is a framework I have prepared for myself containing everything I would include in this chapter. I havent prepared it because I need to find a proper dataset so I can do this data cleaning tutorial example from beginning to the end by using only 1 dataset. It will be like a little project.\n",
    "I know it might look like a lot but datacleaning is the task that takes the most during coding and in my opinion it is nice to have a complete guideline with what to look for and how to deal with those errors before going into coding\n",
    "</span>\n",
    "- I have just written the functions so it will be easier for me when I get the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899704b5",
   "metadata": {},
   "source": [
    "### 1. Look into data (print the first few rows)\n",
    "```\n",
    "df.head()\n",
    "```\n",
    "\n",
    "### 2. Understand the columns and try to analyze if they are in the correct type or not. if not typecast them\n",
    "\n",
    "##### CONVERT INTO A CORRECT FORMAT\n",
    "\n",
    "df.dtypes\n",
    "\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "```\n",
    "##### when you have empty rows and you want to drop them:\n",
    "\n",
    "```\n",
    "df.dropna(subset=['Date'], inplace = True)\n",
    "```\n",
    "\n",
    "\n",
    "### 3. SEPERATE NUMERICAL AND CATEGORICAL COLUMNS\n",
    "\n",
    "seperating them early on is useful \n",
    "\n",
    "\n",
    "```\n",
    "cat_df = airbnb.select_dtypes(include=['object'])\n",
    "num_df = airbnb.select_dtypes(exclude=['object'])\n",
    "\n",
    "def printColumnTypes(non_numeric_df, numeric_df):\n",
    "    '''separates non-numeric and numeric columns'''\n",
    "    print(\"Non-Numeric columns:\")\n",
    "    for col in non_numeric_df:\n",
    "        print(f\"{col}\")\n",
    "    print(\"\")\n",
    "    print(\"Numeric columns:\")\n",
    "    for col in numeric_df:\n",
    "        print(f\"{col}\")\n",
    "        \n",
    "printColumnTypes(cat_df, num_df)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 4. Locate Missing data    .isnull()   a list with boolean values \n",
    "\n",
    "Only print out columns with missing values and shows the amount\n",
    "```\n",
    "df.isnull().sum()\n",
    "```\n",
    "\n",
    "\n",
    "##### Percentage missing \n",
    "\n",
    "```\n",
    "def perc_missing(df):\n",
    "    '''prints out columns with missing values with its %'''\n",
    "    for col in df.columns:\n",
    "        pct = df[col].isna().mean() * 100\n",
    "        if (pct != 0):\n",
    "            print('{} => {}%'.format(col, round(pct, 2)))\n",
    "    \n",
    "perc_missing(airbnb)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Heatmap of missing values\n",
    "\n",
    "Heatmaps are also useful to visualize your missing values, in particular at which point of the data do missing values exists.\n",
    "\n",
    "\n",
    "```\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(airbnb.isnull(), yticklabels=False, cmap='viridis', cbar=False)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "and then analyze \n",
    "\n",
    "#### you can drop the data or input missing data\n",
    "```\n",
    "remove = ['ab','date']\n",
    "data.drop(remove, inplace =True, axis =1)\n",
    "```\n",
    ".dropna()\n",
    "\n",
    "If you want to change the original DataFrame, use the inplace = True argument:\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "df.dropna(inplace = True)\n",
    "```\n",
    "\n",
    "\n",
    "or \n",
    "\n",
    "#### input missing data  by replacing them\n",
    "```\n",
    ".replace({NaN:0.00})\n",
    "```\n",
    "\n",
    "or\n",
    "#### replace with a Scalar Value\n",
    "\n",
    "\n",
    " adding ‘No Review’. When it comes to inputting missing data you can either add ‘No Review’ using the code below, or manually fill in the correct data.\n",
    "\n",
    "INPUT:\n",
    "```\n",
    "data['Review'] = data['Review'].fillna('No review')\n",
    "\n",
    "```\n",
    "A common way to replace empty cells, is to calculate the mean, median or mode value of the column.\n",
    "\n",
    "Pandas uses the mean() median() and mode() methods to calculate the respective values for a specified column:\n",
    "```\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "x = df[\"Calories\"].mean()\n",
    "\n",
    "df[\"Calories\"].fillna(x, inplace = True) \n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "or \n",
    "#### Fill forward or backward\n",
    "\n",
    "bfill/ffill -> filling in missing values based on the value after or before the column\n",
    "```\n",
    "# imputing price with mean\n",
    "price_mean_value = round(airbnb['price'].mean(), 2)\n",
    "airbnb['price'].fillna(price_mean_value, inplace=True)\n",
    "\n",
    "# imputing price with median\n",
    "price_median_value = round(airbnb['price'].median(), 2)\n",
    "airbnb['price'].fillna(price_median_value, inplace=True)\n",
    "\n",
    "# imputing with bfill or ffill\n",
    "airbnb['price'].bfill(inplace=True)\n",
    "airbnb['price'].ffill(inplace=True)\n",
    "\n",
    "# imputing with SimpleImputor from the sklearn library\n",
    "from sklearn.impute import SimpleImputer\n",
    "# define the imputer\n",
    "imr = SimpleImputer(missing_values=np.nan, strategy='mean') # or median\n",
    "\n",
    "airbnb[['price']] = imr.fit_transform(airbnb[['price']])\n",
    "\n",
    "# use strategy = 'most_frequent' for categorical data\n",
    "\n",
    "```\n",
    "```\n",
    ".fillna(method='backfill')\n",
    "\n",
    ".fillna(method='pad')\n",
    "\n",
    "#for a specific column\n",
    "\n",
    "df[\"Calories\"].fillna(130, inplace = True) \n",
    "```\n",
    "### 5. Check for duplicates\n",
    "```\n",
    "df.duplicate()\n",
    "```\n",
    "\n",
    "#### and then remove duplicates\n",
    "```\n",
    "df.drop_duplicates()\n",
    "```\n",
    "\n",
    "### 6.Detect Outliers\n",
    "- so you can remove it\n",
    "```\n",
    "data['Rating'].describe()\n",
    "```\n",
    "\n",
    "Take a look at that ‘max’ value - none of the other values are even close to 100, with the mean (the average) being 11. Now, your solution to outliers will depend on your knowledge of your dataset. In this case, the data scientists who input the knowledge know that they meant to put a value of 1 not 100. So, we can safely remove the outlier to fix our data.\n",
    "\n",
    "data.loc[10,'Rating'] = 1 \n",
    "(Now our dataset has ratings ranging from 1 to 5, which will save major skew from if there was a rogue 100 in there.)\n",
    "\n",
    "\n",
    "### 7.Normalize Casing\n",
    "\n",
    "#### Standardize (lowercase) or uppercase\n",
    "```\n",
    "    data['Review Title'] = data['Review Title'].str.lower()\n",
    "    data['Customer Name'] = data['Customer Name'].str.title()\n",
    "```  \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "Should we include array operations with np such as flatten(), reshape()\n",
    "\n",
    "\n",
    "### 8. Rename Columns \n",
    "```\n",
    ".rename(columns={'a':'Date','B':'Time}, inplace=True)\n",
    "```\n",
    "\n",
    "the changes that we have made to the frames did not actually modify them. To make this happen, you can set the inplace=True parameter\n",
    "\n",
    "\n",
    "### 9. Values that doesnt make sense\n",
    "for example to have 1600 minutes as a workout time of a person\n",
    "\n",
    "#### you can replace them\n",
    "```\n",
    "for x in df.index:\n",
    "  if df.loc[x, \"Duration\"] > 120:\n",
    "    df.loc[x, \"Duration\"] = 120\n",
    "```\n",
    "\n",
    "#### or delete them\n",
    "\n",
    "```\n",
    "for x in df.index:\n",
    "  if df.loc[x, \"Duration\"] > 120:\n",
    "    df.drop(x, inplace = True) \n",
    "```\n",
    "\n",
    "\n",
    "### 10. If you have columns of strings, check for trailing whitespaces\n",
    "\n",
    "After we know which data types we are dealing with, let’s make sure we remove any trailing characters and whitespace using strip .\n",
    "\n",
    "\n",
    "```\n",
    "# getting all the columns with string/mixed type values\n",
    "str_cols = list(netflix_titles.columns)\n",
    "str_cols.remove('release_year')\n",
    "\n",
    "# removing leading and trailing characters from columns with str type\n",
    "for i in str_cols:\n",
    "    netflix_titles[i] = netflix_titles[i].str.strip()\n",
    "```\n",
    "\n",
    "### 11. Check the unique values of columns\n",
    "\n",
    "Beyond potentially missing values, there could be corrupted values that you can run into once you perform analysis. To check this, we can check for unique values for some of the columns\n",
    "\n",
    "```\n",
    "netflix_films['rating'].unique()\n",
    "\n",
    "```\n",
    "\n",
    "https://towardsdatascience.com/how-to-clean-your-data-in-python-8f178638b98d (maybe go and see how she/he explained this point here)\n",
    "\n",
    "\n",
    "### 12. Spelling Errors in Categorical Data\n",
    "\n",
    "\n",
    "if you are dealing with alrogithms for categorization \"Berlin\" is not the same as \"berlin\"\n",
    "\n",
    "\n",
    "```\n",
    "random_index = airbnb.sample(2, random_state = 10).index\n",
    "\n",
    "# airbnb['neighbourhood_group'].loc[random_index]\n",
    "## we randomly selected Manhattan and Brooklyn\n",
    "\n",
    "wrong_spelling = ['manhatann', 'brookln']\n",
    "\n",
    "# replace them with the wrong spelling\n",
    "airbnb.loc[random_index,'neighbourhood_group'] = wrong_spelling\n",
    "airbnb['neighbourhood_group'].value_counts()\n",
    "```\n",
    "\n",
    "### 13.REGEX EXPRESSIONS FOR CLeANING but this might be a lot \n",
    "\n",
    "### 14. Outliers Detection  \n",
    "check t for histogram and boxplot approach \n",
    " it is also included at the 6th point but here you can have a detailed approach of how to deal with them. this topic is helpful even for modeling\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1045bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "<span style=\"color:darkred\"> :Question: Should we split it into subchapters or do you have any other suggestion?  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7067e856",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3312af7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
